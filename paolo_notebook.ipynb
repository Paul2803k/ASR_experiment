{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASR: Experiment 3, Whisper vs Wav2Vec2 vs Vosk"
      ],
      "metadata": {
        "id": "-Zr6ZTbGMOFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shared variables\n",
        "\n",
        "# indicates how many lines of the data sets we should go through (max for the drive is 3994)\n",
        "LIMIT = 20\n",
        "# indicates if we need to install the vosk model (since it takes a bit, 1.8 GB)\n",
        "INSTALL = False\n",
        "# indicates if we need to convert the files from .mp3 to .wav for Vosk\n",
        "CONVERT = False\n",
        "# indicates if we want to perform a clean run (get rid of everything installed for Vosk)\n",
        "CLEAN = False"
      ],
      "metadata": {
        "id": "sm2fBwJrVca-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up functions for a clean run, plus removes th egarbage from the installation\n",
        "import shutil\n",
        "\n",
        "if CLEAN:\n",
        "  try:\n",
        "    shutil.rmtree('/content/vosk-model-en-us-0.22')\n",
        "    shutil.rmtree('/content/vosk-api')\n",
        "    shutil.rmtree('/content/sample_data')\n",
        "  except Exception:\n",
        "    print(\"No folder to clean, continuing..\")"
      ],
      "metadata": {
        "id": "UF_W4Yt1Uscz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shared install\n",
        "!apt install ffmpeg\n",
        "!pip install tqdm\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "lH8cm51PHxN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shared imports\n",
        "from os import path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "nsXkvDc6G_ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the drive with the dataset\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6vWI3QDoneiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading in the dataset csv\n",
        "# data keys are: filename, text, up_votes, down_votes, age, gender, accent and duration\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/cv-valid-test.csv\")"
      ],
      "metadata": {
        "id": "zda118auiipx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shared utility functions\n",
        "\n",
        "# gets short, average and long keywords in a sentence\n",
        "def grab_keywords(sentence):\n",
        "    words = sentence.split(\" \")\n",
        "    keywords = [[],[],[]]\n",
        "    for i in words:\n",
        "        if len(i) <= 3:\n",
        "            keywords[0].append(i)\n",
        "    for i in words:\n",
        "        if len(i) == 4 or len(i) == 5:\n",
        "            keywords[1].append(i)\n",
        "    for i in words:\n",
        "        if len(i) > 5:\n",
        "            keywords[2].append(i)\n",
        "    return [\" \".join(keywords[0]),\" \".join(keywords[1]),\" \".join(keywords[2])]\n",
        "\n",
        "# mitch scoring stuff\n",
        "def cosinesimularity(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "    sum1 = sum([vec1[x]**2 for x in list(vec1.keys())])\n",
        "    sum2 = sum([vec2[x]**2 for x in list(vec2.keys())])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def text_to_vector(text):\n",
        "    WORD = re.compile(r\"\\w+\")\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "def get_cosinesimularity(text1, text2):\n",
        "    if text1 == '' and text2 == '':\n",
        "      return -1\n",
        "    vector1 = text_to_vector(text1.lower().translate(str.maketrans('', '', string.punctuation)))\n",
        "    vector2 = text_to_vector(text2.lower().translate(str.maketrans('', '', string.punctuation)))\n",
        "    cosine = cosinesimularity(vector1, vector2)\n",
        "    return cosine\n",
        "\n",
        "# used for final prints\n",
        "def get_label(index):\n",
        "    if index == 0:\n",
        "      return 'short'\n",
        "    if index == 1:\n",
        "      return 'average'\n",
        "    if index == 2:\n",
        "      return 'long'"
      ],
      "metadata": {
        "id": "EfFPZVbxk3q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Whisper**"
      ],
      "metadata": {
        "id": "bAt6quvVP7as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper imports\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration"
      ],
      "metadata": {
        "id": "q9SATwJCHUKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model and processor\n",
        "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
        "whisper_med_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base.en\")"
      ],
      "metadata": {
        "id": "QzG-B8jPHT52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER FULL TRANSCRIPT SCORE\n",
        "\n",
        "scores = []\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "  # limiter for testing\n",
        "  if index == 20:\n",
        "    break\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "\n",
        "  # load the sample\n",
        "  waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "  # whisper only works with 16k sample rate so if we have others we convert it\n",
        "  if sample_rate != 16000:\n",
        "     waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "  # tokenize\n",
        "  input_features = whisper_processor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "\n",
        "  # generate token ids\n",
        "  predicted_ids = whisper_med_model.generate(input_features)\n",
        "\n",
        "  # decode token ids to text\n",
        "  transcript = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "  text = samp[1][1]\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(text)\n",
        "  # print(transcript)\n",
        "\n",
        "  # filling scores\n",
        "  score = get_cosinesimularity(text, transcript)\n",
        "  scores.append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "print(\"Mean score for full sentences: {}\".format(np.mean(scores)))\n",
        "print(\"[*] Median scores\")\n",
        "print(\"Median score for full sentences: {}\".format(np.median(scores)))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "6ByB8iM6HTqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER KEYWORDS SCORE (<=3 && 4,5 && >5)\n",
        "\n",
        "scores = [[],[],[]]\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "  # limiter for testing\n",
        "  if index == LIMIT:\n",
        "    break\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "\n",
        "  # load the sample\n",
        "  waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "  # whisper only works with 16k sample rate so if we have others we convert it\n",
        "  if sample_rate != 16000:\n",
        "     waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "  # tokenize\n",
        "  input_features = whisper_processor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "\n",
        "  # generate token ids\n",
        "  predicted_ids = whisper_med_model.generate(input_features)\n",
        "\n",
        "  # decode token ids to text\n",
        "  transcript = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "  text = samp[1][1]\n",
        "\n",
        "  # get keywords of different length\n",
        "  keywords_tr = grab_keywords(transcript)\n",
        "  keywords_te = grab_keywords(text)\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(keywords_tr)\n",
        "  # print(keywords_te)\n",
        "\n",
        "  # filling scores\n",
        "  for i in range(3):\n",
        "    score = get_cosinesimularity(keywords_tr[i], keywords_te[i])\n",
        "    if score != -1:\n",
        "      scores[i].append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "for i in range(3):\n",
        "    print(\"Mean score for {} words: {}\".format(get_label(i), np.mean(scores[i])))\n",
        "print(\"[*] Median scores\")\n",
        "for i in range(3):\n",
        "    print(\"Median score for {} words: {}\".format(get_label(i), np.median(scores[i])))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "lFCXQi0kKHKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wav2vec**"
      ],
      "metadata": {
        "id": "K8GHQ-jaPkub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC"
      ],
      "metadata": {
        "id": "tn6cCBQImQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load processor and model\n",
        "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "id": "dMWhWYP83Bid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC FULL TRANSCRIPT SCORE\n",
        "\n",
        "scores = []\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "  # limiter for testing\n",
        "  if index == LIMIT:\n",
        "    break\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "\n",
        "  # load the sample\n",
        "  waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "  # wav2vec only works with 16k sample rate so if we have others we convert it\n",
        "  if sample_rate != 16000:\n",
        "     waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "  # tokenize\n",
        "  input_values = wav2vec_processor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "\n",
        "  # retrieve logits\n",
        "  logits = wav2vec_model(input_values).logits\n",
        "\n",
        "  # take argmax and decode\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  transcript = wav2vec_processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "  text = samp[1][1]\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(text)\n",
        "  # print(transcript)\n",
        "\n",
        "  # filling scores\n",
        "  score = get_cosinesimularity(text, transcript)\n",
        "  scores.append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "print(\"Mean score for full sentences: {}\".format(np.mean(scores)))\n",
        "print(\"[*] Median scores\")\n",
        "print(\"Median score for full sentences: {}\".format(np.median(scores)))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "dLuDaQ-y8_HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC KEYWORDS SCORE (<=3 && 4,5 && >5)\n",
        "\n",
        "scores = [[],[],[]]\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "  # limiter for testing\n",
        "  if index == LIMIT:\n",
        "    break\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "  # load the sample\n",
        "  waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "  # wav2vec only works with 16k sample rate so if we have others we convert it\n",
        "  if sample_rate != 16000:\n",
        "     waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "  # tokenize\n",
        "  input_values = wav2vec_processor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "\n",
        "  # retrieve logits\n",
        "  logits = wav2vec_model(input_values).logits\n",
        "\n",
        "  # take argmax and decode\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  transcript = wav2vec_processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "  text = samp[1][1]\n",
        "\n",
        "  # get keywords of different length\n",
        "  keywords_tr = grab_keywords(transcript)\n",
        "  keywords_te = grab_keywords(text)\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(keywords_tr)\n",
        "  # print(keywords_te)\n",
        "\n",
        "  # filling scores\n",
        "  for i in range(3):\n",
        "    score =  get_cosinesimularity(keywords_tr[i], keywords_te[i])\n",
        "    if score != -1:\n",
        "      scores[i].append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "for i in range(3):\n",
        "    print(\"Mean score for {} words: {}\".format(get_label(i), np.mean(scores[i])))\n",
        "print(\"[*] Median scores\")\n",
        "for i in range(3):\n",
        "    print(\"Median score for {} words: {}\".format(get_label(i), np.median(scores[i])))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "BpR6_IYqNx-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vosk**"
      ],
      "metadata": {
        "id": "AgLP7s_7SuRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install vosk api and pydub [once per session]\n",
        "!pip install vosk\n",
        "# !git clone https://github.com/alphacep/vosk-api\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "EJ4U0fJeHBuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get model [once per session]\n",
        "if INSTALL:\n",
        "  !wget https://alphacephei.com/kaldi/models/vosk-model-en-us-0.22.zip\n",
        "  !unzip vosk-model-en-us-0.22.zip\n",
        "  %mv vosk-model-en-us-0.22"
      ],
      "metadata": {
        "id": "I5iN8bOIHDn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from pydub import AudioSegment\n",
        "import wave\n",
        "import json"
      ],
      "metadata": {
        "id": "N2qSW9_iHD0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert files for experiments [one time thing]\n",
        "if CONVERT:\n",
        "  for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "    # get full path of sample\n",
        "    file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "    # destination path for sample\n",
        "    dest = file_path.replace('.mp3', '.wav')\n",
        "    # we skip the ones already converted\n",
        "    if path.isfile(dest):\n",
        "      continue\n",
        "    # convert wav to mp3, vosk only uses WAV mono PCM\n",
        "    sound = AudioSegment.from_mp3(file_path)\n",
        "    sound.export(dest, format=\"wav\")\n",
        "    print()\n",
        "    print(\"Converted {} to {}\".format(file_path, dest))"
      ],
      "metadata": {
        "id": "S6y8d8A_3Epo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import model\n",
        "vosk_model = Model(\"/content/vosk-model-en-us-0.22\")"
      ],
      "metadata": {
        "id": "PyD228se_s1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VOSK FULL TRANSCRIPT SCORE\n",
        "scores = []\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "  # limiter for testing\n",
        "  if index == LIMIT:\n",
        "    break\n",
        "\n",
        "  # get full path of sample\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "\n",
        "  # destination path for sample\n",
        "  dest = file_path.replace('.mp3', '.wav')\n",
        "\n",
        "  # open file\n",
        "  wf = wave.open(dest,\"rb\")\n",
        "\n",
        "  # check if the conversion is proper\n",
        "  if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "    exit (1)\n",
        "\n",
        "  # initialise recogniser\n",
        "  rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "\n",
        "  # recognition loop\n",
        "  while True:\n",
        "    d = wf.readframes(4000)\n",
        "    if len(d) == 0:\n",
        "        break\n",
        "    rec.AcceptWaveform(d)\n",
        "\n",
        "  text = samp[1].text\n",
        "  transcript = json.loads(rec.FinalResult())[\"text\"]\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(text)\n",
        "  # print(transcript)\n",
        "\n",
        "  # filling scores\n",
        "  score =  get_cosinesimularity(transcript, text)\n",
        "  scores.append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "print(\"Mean score for full sentences: {}\".format(np.mean(scores)))\n",
        "print(\"[*] Median scores\")\n",
        "print(\"Median score for full sentences: {}\".format(np.median(scores)))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "cTDEBaBAHEBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VOSK KEYWORDS SCORE (<=3 && 4,5 && >5)\n",
        "\n",
        "scores = [[],[],[]]\n",
        "\n",
        "print(\"[!] Starting experiment\")\n",
        "\n",
        "for index, samp in enumerate(tqdm(data.iterrows())):\n",
        "\n",
        "  # limiter for testing\n",
        "  if index == LIMIT:\n",
        "    break\n",
        "\n",
        "  # get full path of sample\n",
        "  file_path = \"/content/drive/MyDrive/\"+samp[1][0]\n",
        "\n",
        "  # destination path for sample\n",
        "  dest = file_path.replace('.mp3', '.wav')\n",
        "\n",
        "  # open file\n",
        "  wf = wave.open(dest,\"rb\")\n",
        "\n",
        "  # check if the conversion is proper\n",
        "  if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "    exit (1)\n",
        "\n",
        "  # initialise recogniser\n",
        "  rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "\n",
        "  # recognition loop\n",
        "  while True:\n",
        "    d = wf.readframes(4000)\n",
        "    if len(d) == 0:\n",
        "        break\n",
        "    rec.AcceptWaveform(d)\n",
        "\n",
        "  text = samp[1].text\n",
        "  transcript = json.loads(rec.FinalResult())[\"text\"]\n",
        "\n",
        "  # get keywords of different length\n",
        "  keywords_tr = grab_keywords(transcript)\n",
        "  keywords_te = grab_keywords(text)\n",
        "\n",
        "  # debug print\n",
        "  print()\n",
        "  # print(keywords_tr)\n",
        "  # print(keywords_te)\n",
        "\n",
        "  # filling scores\n",
        "  for i in range(3):\n",
        "    score =  get_cosinesimularity(keywords_tr[i], keywords_te[i])\n",
        "    if score != -1:\n",
        "      scores[i].append(score)\n",
        "\n",
        "print(\"[!] End of experiment, aggregating scores\")\n",
        "\n",
        "print(\"[*] Mean scores\")\n",
        "for i in range(3):\n",
        "    print(\"Mean score for {} words: {}\".format(get_label(i), np.mean(scores[i])))\n",
        "print(\"[*] Median scores\")\n",
        "for i in range(3):\n",
        "    print(\"Median score for {} words: {}\".format(get_label(i), np.median(scores[i])))\n",
        "\n",
        "print(\"[!] Complete\")"
      ],
      "metadata": {
        "id": "DTaVJTatulGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}